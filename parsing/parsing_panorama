from bs4 import BeautifulSoup 
import requests
import csv


news_file = open('news_panorama_society_401_671.csv', 'w', newline = '', encoding = 'Windows-1251')

topic_list = ['science', 'society', 'economics']
number_of_pages = [46, 672, 49]

    
for m in range(0, 1):
    base_link = 'https://panorama.pub/society?page='
    new_base_link = 'https://panorama.pub'
    for i in range(401, 672):
        r = requests.get(base_link + str(i))
        soup = BeautifulSoup(r.content, features='html.parser')
        news = soup.findAll('a', {'class': 'entry big'})
        for new in news:
            article_text = ''
            article_info = []
            link = new.get('href')
            piece_news = requests.get(new_base_link + link)
            piece_soup = BeautifulSoup(piece_news.content, features='html.parser')
            
            news_title = piece_soup.find('title').text

            topic = piece_soup.find('span', {'class': 'badge badge-dark'}).text

            date = piece_soup.find('span', {'class': 'date'}).text
            date = date[2:]
            
            article = piece_soup.find('div', {'class': 'entry-contents'})
            paras = article.findAll('p')
            for para in paras:
                article_text += '<p>' + para.text + '</p>'

            article_info = [new_base_link + link, news_title, topic, date, article_text]
            #news_base.append(article_info)
            print(i)

            try:
                writer = csv.writer(news_file, delimiter = '|')
                writer.writerow(article_info)
            except:
                continue

            
news_file.close()  
